{"cells":[{"cell_type":"markdown","source":["# **Step-1:** Importing Required Libraries"],"metadata":{"id":"6kBSfklNcqHQ"}},{"cell_type":"code","source":["!pip3 install -U tensorflow-addons\n"],"metadata":{"id":"7Tpb39Ex3DaQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668247545707,"user_tz":-330,"elapsed":4927,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}},"outputId":"fad1524d-1fbf-489c-baa7-a52b2997ee98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.18.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWHwDY4pVOQQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668247572239,"user_tz":-330,"elapsed":22237,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}},"outputId":"7bb26f1b-0da7-4f93-b61b-7cfedf87d3e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrBkNhYgLTL0"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","import glob\n","import cv2\n","import os"],"metadata":{"id":"Huo-rGxn4DXQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-2:** Importing Data"],"metadata":{"id":"mDgKskeBetz1"}},{"cell_type":"code","source":["import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","#Capture training data and labels into respective lists\n","images = []\n","labels = []\n","\n","\n","for directory_path in glob.glob(\"/content/drive/MyDrive/NIT Durgapur Internship 3/Dataset_Modified/*\"):\n","   data_split=os.path.split(directory_path)\n","   label=data_split[-1]\n","   print(label)"],"metadata":{"id":"sxoduhjy8f1Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668247583268,"user_tz":-330,"elapsed":989,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}},"outputId":"fc9136e7-8e50-4a4b-a50e-93518221a9e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hand\n","BreastMRI\n","ChestCT\n","CXR\n","AbdomenCT\n","HeadCT\n"]}]},{"cell_type":"code","source":["CLASSES = os.listdir('/content/drive/MyDrive/NIT Durgapur Internship 3/Dataset_Modified')\n","TRAINING_DATA_SET_PATH = '/content/drive/MyDrive/NIT Durgapur Internship 3/Dataset_Modified'\n","TEST_DATA_SET_PATH = '/content/drive/MyDrive/NIT Durgapur Internship 3/Dataset_for_test'"],"metadata":{"id":"2Bg-UARF8fzY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-3:** Configuring the hyperparameters"],"metadata":{"id":"HptlTjnaZl6P"}},{"cell_type":"code","source":["params = dict(\n","    seed = 123,\n","    image_dim = (288,288),\n","    weight_decay = 1e-4,\n","    epochs = 30,\n","    batch_size = 16,\n","    patch_size = 18,\n","    pool_size = (2,2),\n","    optimizer = 'Adam',\n","    l_rate = 0.001,\n","    val_split = .15,\n","    use_transfer_learning = False,\n","    use_data_aug = False,\n","\n","    l2_reg = .0,\n","    projection_dim = 16,\n","    num_heads = 4,\n","\n","    # Size of the transformer layers\n","    transformer_layers = 4,\n","    num_classes = len(CLASSES),\n","    mlp_head_units = [1024,512]\n","\n","    )"],"metadata":{"id":"k47wK1H78fwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_params = dict(\n","    num_patches = (params['image_dim'][0] // params['patch_size']) ** 2,\n","    transformer_units = [\n","    params['projection_dim'] * 2,\n","    params['projection_dim']],\n","    input_shape = (params['image_dim'][0], params['image_dim'][1], 3),\n","\n",")\n","params.update(new_params)"],"metadata":{"id":"I6Zzgic-8fuI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # **Step-4:** Data augmentation"],"metadata":{"id":"Ppqa4aXvZZMe"}},{"cell_type":"code","source":["if params['use_data_aug']:\n","  data_aug_params = dict(\n","      da_rotation = 20,\n","      da_w_shift = 0.1,\n","      da_h_shift = 0.1,\n","      da_shear = 0.05,\n","      da_zoom = 0.05,\n","      da_h_flip = False,\n","      da_v_flip = False,\n","  )\n","\n","  params.update(data_aug_params)"],"metadata":{"id":"SiR2TwOY8frR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ability to switch amount of channel to utilise pre-trained models with specific input shapes\n","if params['use_transfer_learning']:\n","  INPUT_SHAPE = (params['image_dim'][0], params['image_dim'][1], 3)\n","  COLOUR_MODE = 'rgb'\n","else:\n","  INPUT_SHAPE = (params['image_dim'][0], params['image_dim'][1], 3)\n","  COLOUR_MODE = 'rgb'\n"],"metadata":{"id":"VjIDugz5MksK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator"],"metadata":{"id":"cvaaD0T8M7o4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if params['use_data_aug']:\n","  datagen = ImageDataGenerator(validation_split=params['val_split'], rescale=1./255,\n","                               rotation_range=params['da_rotation'],\n","                               width_shift_range=params['da_w_shift'],\n","                               height_shift_range=params['da_h_shift'],\n","                               shear_range=params['da_shear'],\n","                               zoom_range=params['da_zoom'],\n","                               horizontal_flip=params['da_h_flip'],\n","                               vertical_flip=params['da_v_flip'],\n","                               fill_mode=\"constant\",\n","                               cval=0\n","                               )\n","else:\n","  datagen = ImageDataGenerator(validation_split=params['val_split'])"],"metadata":{"id":"ucF_OSA7Mkox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read all training and validation data into variables from directory.\n","# Due to faulty quality of the given validation-set images, all images are taken from the training folder\n","train_generator = datagen.flow_from_directory(TRAINING_DATA_SET_PATH,\n","                                                    batch_size=params['batch_size'],\n","                                                    seed=123,\n","                                                    class_mode=\"categorical\",\n","                                                    classes=CLASSES,\n","                                                    target_size=params['image_dim'],\n","                                                    color_mode=COLOUR_MODE,\n","                                                    subset='training',\n","                                                    shuffle=True)\n","\n","val_datagen = ImageDataGenerator(validation_split=0.15, rescale=1./255)\n","valid_generator = datagen.flow_from_directory(TRAINING_DATA_SET_PATH,\n","                                                    batch_size=params['batch_size'],\n","                                                    seed=123,\n","                                                    class_mode=\"categorical\",\n","                                                    classes=CLASSES,\n","                                                    target_size=params['image_dim'],\n","                                                    color_mode=COLOUR_MODE,\n","                                                    subset='validation',\n","                                                    shuffle=False)\n"],"metadata":{"id":"BF2W4XX4Mklt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668247632856,"user_tz":-330,"elapsed":26063,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}},"outputId":"c624e61d-dec4-485f-88df-1dc95d543c05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 22967 images belonging to 6 classes.\n","Found 4053 images belonging to 6 classes.\n"]}]},{"cell_type":"code","source":["test_datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(TEST_DATA_SET_PATH,\n","                                                    batch_size=params['batch_size'],\n","                                                    seed=123,\n","                                                    class_mode=\"categorical\",\n","                                                    classes=CLASSES,\n","                                                    target_size=params['image_dim'],\n","                                                    color_mode=COLOUR_MODE,\n","                                                    shuffle=False)"],"metadata":{"id":"rtZS1CLxMkjR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668247650232,"user_tz":-330,"elapsed":17384,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}},"outputId":"ccda5b5f-3a90-496f-ebc1-c9b3292e4fc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 11580 images belonging to 6 classes.\n"]}]},{"cell_type":"markdown","source":["# **Step-5:** Building MLP network"],"metadata":{"id":"uas5bXXdaBjm"}},{"cell_type":"code","source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"],"metadata":{"id":"fZmds2bOMkhR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-6:** Defining Patch maker"],"metadata":{"id":"FWitW8UwdNbZ"}},{"cell_type":"code","source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches"],"metadata":{"id":"htMchJ8WMkek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-7:** Patch encoder"],"metadata":{"id":"0744dqp6dY9Y"}},{"cell_type":"code","source":["# Linearly transform patches by projecting it into a\n","# vector of size `projection_dim` and also adds a learnable position\n","# embedding to the projected vector.\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded"],"metadata":{"id":"pYHCLs77MkcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(4, 4))\n","image, label = iter(next(train_generator))\n","image = image[0]*255.\n","#image = x_train[np.random.choice(range(x_train.shape[0]))]\n","plt.imshow((image).astype(\"uint8\"))\n","plt.axis(\"off\")\n","\n","resized_image = tf.image.resize(\n","    tf.convert_to_tensor([image]), size=(params['image_dim'][0], params['image_dim'][0])\n",")\n","patches = Patches(params['patch_size'])(resized_image)\n","print(f\"Image size: {params['image_dim'][0]} X {params['image_dim'][0]}\")\n","print(f\"Patch size: {params['patch_size']} X {params['patch_size']}\")\n","print(f\"Patches per image: {patches.shape[1]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (params['patch_size'], params['patch_size'], 3))\n","    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"],"metadata":{"id":"KV29RmHCN5W2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-8:** Generate ViT model\n"],"metadata":{"id":"xytIzRmTfQOu"}},{"cell_type":"code","source":["def create_vit_classifier():\n","    inputs = layers.Input(shape=params['input_shape'])\n","    # Create patches.\n","    patches = Patches(params['patch_size'])(inputs)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(params['num_patches'], params['projection_dim'])(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(params['transformer_layers']):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=params['num_heads'], key_dim=params['projection_dim'], dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=params['transformer_units'], dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","# Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=params['mlp_head_units'], dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(len(CLASSES))(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"],"metadata":{"id":"AKFiprGGMkZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step-9:** Compiling & Training the Model\n"],"metadata":{"id":"RoPcGEKKfrSW"}},{"cell_type":"code","source":["!pip3 install wandb==0.10.11"],"metadata":{"id":"V7gD9neqT2EW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K\n","from wandb.keras import WandbCallback"],"metadata":{"id":"cMPtBruaVnNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# For experiment tracking\n","USE_WANDB = False\n","\n","if USE_WANDB:\n","  wandb.init(tags=[\"ViT\", \"Binary\"], project='dd2424', entity='teambumblebee', sync_tensorboard=True, config=params, save_code=True)\n","  print(params)\n","  wandb_callback = WandbCallback(monitor='val_f1_m',\n","                                 save_model=True,\n","                                 save_weights_only=False, mode='max',\n","                                 log_weights=True,\n","                                 data_type=\"image\", verbose=1,\n","                                 labels=CLASSES,\n","                                 generator=valid_generator,\n","                                 predictions=50,\n","                                 log_evaluation=True,\n","                                 log_batch_frequency=1)\n","\n","def run_experiment(model):\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=params['l_rate'], weight_decay=params['weight_decay']\n","    )\n","\n","    def recall_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","    def precision_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","    def f1_m(y_true, y_pred):\n","        precision = precision_m(y_true, y_pred)\n","        recall = recall_m(y_true, y_pred)\n","        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.BinaryAccuracy(name=\"accuracy\"), f1_m, recall_m, precision_m\n","        ],\n","    )\n","\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        \"/content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\",\n","        monitor=\"val_f1_m\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","        verbose=1,\n","        mode=\"max\"\n","    )\n","\n","    callbacks_list = [checkpoint_callback, tf.keras.callbacks.EarlyStopping(patience=20, monitor=\"val_accuracy\")]\n","\n","    if USE_WANDB:\n","      callbacks_list.append(wandb_callback)\n","\n","\n","    history = model.fit(\n","        train_generator,\n","        batch_size=params['batch_size'],\n","        epochs=params['epochs'],\n","        validation_data=valid_generator,\n","        callbacks=callbacks_list)\n","\n","    return history, model\n","\n","\n"],"metadata":{"id":"nA1D1CKBOSKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = create_vit_classifier()\n","history, model = run_experiment(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOW0wMpOOSHN","outputId":"24ff6548-8b33-4762-e43c-ed58a3ec9caa","executionInfo":{"status":"ok","timestamp":1668286387815,"user_tz":-330,"elapsed":31414638,"user":{"displayName":"SRJ PGS","userId":"10913169226115545343"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9408 - f1_m: 0.7804 - recall_m: 0.7196 - precision_m: 0.8926\n","Epoch 1: val_f1_m improved from -inf to 0.89943, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1056s 730ms/step - loss: 0.1527 - accuracy: 0.9408 - f1_m: 0.7804 - recall_m: 0.7196 - precision_m: 0.8926 - val_loss: 0.0707 - val_accuracy: 0.9681 - val_f1_m: 0.8994 - val_recall_m: 0.8907 - val_precision_m: 0.9101\n","Epoch 2/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9907 - f1_m: 0.9713 - recall_m: 0.9633 - precision_m: 0.9802\n","Epoch 2: val_f1_m improved from 0.89943 to 0.93657, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1039s 724ms/step - loss: 0.0285 - accuracy: 0.9907 - f1_m: 0.9713 - recall_m: 0.9633 - precision_m: 0.9802 - val_loss: 0.0527 - val_accuracy: 0.9787 - val_f1_m: 0.9366 - val_recall_m: 0.9363 - val_precision_m: 0.9370\n","Epoch 3/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9939 - f1_m: 0.9813 - recall_m: 0.9768 - precision_m: 0.9863\n","Epoch 3: val_f1_m improved from 0.93657 to 0.94698, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1047s 729ms/step - loss: 0.0187 - accuracy: 0.9939 - f1_m: 0.9813 - recall_m: 0.9768 - precision_m: 0.9863 - val_loss: 0.0473 - val_accuracy: 0.9821 - val_f1_m: 0.9470 - val_recall_m: 0.9471 - val_precision_m: 0.9470\n","Epoch 4/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9947 - f1_m: 0.9839 - recall_m: 0.9801 - precision_m: 0.9880\n","Epoch 4: val_f1_m improved from 0.94698 to 0.96094, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1054s 734ms/step - loss: 0.0162 - accuracy: 0.9947 - f1_m: 0.9839 - recall_m: 0.9801 - precision_m: 0.9880 - val_loss: 0.0379 - val_accuracy: 0.9873 - val_f1_m: 0.9609 - val_recall_m: 0.9584 - val_precision_m: 0.9637\n","Epoch 5/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9952 - f1_m: 0.9855 - recall_m: 0.9820 - precision_m: 0.9893\n","Epoch 5: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1043s 726ms/step - loss: 0.0142 - accuracy: 0.9952 - f1_m: 0.9855 - recall_m: 0.9820 - precision_m: 0.9893 - val_loss: 0.0608 - val_accuracy: 0.9798 - val_f1_m: 0.9408 - val_recall_m: 0.9422 - val_precision_m: 0.9395\n","Epoch 6/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9967 - f1_m: 0.9899 - recall_m: 0.9879 - precision_m: 0.9922\n","Epoch 6: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1042s 725ms/step - loss: 0.0107 - accuracy: 0.9967 - f1_m: 0.9899 - recall_m: 0.9879 - precision_m: 0.9922 - val_loss: 0.0488 - val_accuracy: 0.9827 - val_f1_m: 0.9478 - val_recall_m: 0.9466 - val_precision_m: 0.9492\n","Epoch 7/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9962 - f1_m: 0.9884 - recall_m: 0.9859 - precision_m: 0.9912\n","Epoch 7: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1046s 728ms/step - loss: 0.0119 - accuracy: 0.9962 - f1_m: 0.9884 - recall_m: 0.9859 - precision_m: 0.9912 - val_loss: 0.1114 - val_accuracy: 0.9722 - val_f1_m: 0.9154 - val_recall_m: 0.9141 - val_precision_m: 0.9168\n","Epoch 8/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9967 - f1_m: 0.9901 - recall_m: 0.9879 - precision_m: 0.9925\n","Epoch 8: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1034s 720ms/step - loss: 0.0113 - accuracy: 0.9967 - f1_m: 0.9901 - recall_m: 0.9879 - precision_m: 0.9925 - val_loss: 0.2163 - val_accuracy: 0.9589 - val_f1_m: 0.8758 - val_recall_m: 0.8750 - val_precision_m: 0.8767\n","Epoch 9/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9967 - f1_m: 0.9900 - recall_m: 0.9878 - precision_m: 0.9923\n","Epoch 9: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1035s 721ms/step - loss: 0.0100 - accuracy: 0.9967 - f1_m: 0.9900 - recall_m: 0.9878 - precision_m: 0.9923 - val_loss: 0.0902 - val_accuracy: 0.9759 - val_f1_m: 0.9264 - val_recall_m: 0.9245 - val_precision_m: 0.9284\n","Epoch 10/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976 - f1_m: 0.9928 - recall_m: 0.9913 - precision_m: 0.9946\n","Epoch 10: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1029s 716ms/step - loss: 0.0070 - accuracy: 0.9976 - f1_m: 0.9928 - recall_m: 0.9913 - precision_m: 0.9946 - val_loss: 0.1080 - val_accuracy: 0.9711 - val_f1_m: 0.9119 - val_recall_m: 0.9107 - val_precision_m: 0.9133\n","Epoch 11/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975 - f1_m: 0.9925 - recall_m: 0.9911 - precision_m: 0.9941\n","Epoch 11: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1028s 716ms/step - loss: 0.0079 - accuracy: 0.9975 - f1_m: 0.9925 - recall_m: 0.9911 - precision_m: 0.9941 - val_loss: 0.0435 - val_accuracy: 0.9852 - val_f1_m: 0.9553 - val_recall_m: 0.9552 - val_precision_m: 0.9555\n","Epoch 12/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9973 - f1_m: 0.9919 - recall_m: 0.9906 - precision_m: 0.9934\n","Epoch 12: val_f1_m did not improve from 0.96094\n","1436/1436 [==============================] - 1030s 718ms/step - loss: 0.0079 - accuracy: 0.9973 - f1_m: 0.9919 - recall_m: 0.9906 - precision_m: 0.9934 - val_loss: 0.1067 - val_accuracy: 0.9762 - val_f1_m: 0.9290 - val_recall_m: 0.9286 - val_precision_m: 0.9295\n","Epoch 13/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9975 - f1_m: 0.9925 - recall_m: 0.9911 - precision_m: 0.9940\n","Epoch 13: val_f1_m improved from 0.96094 to 0.96991, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1031s 718ms/step - loss: 0.0075 - accuracy: 0.9975 - f1_m: 0.9925 - recall_m: 0.9911 - precision_m: 0.9940 - val_loss: 0.0239 - val_accuracy: 0.9904 - val_f1_m: 0.9699 - val_recall_m: 0.9651 - val_precision_m: 0.9755\n","Epoch 14/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978 - f1_m: 0.9934 - recall_m: 0.9921 - precision_m: 0.9949\n","Epoch 14: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1029s 717ms/step - loss: 0.0067 - accuracy: 0.9978 - f1_m: 0.9934 - recall_m: 0.9921 - precision_m: 0.9949 - val_loss: 0.0875 - val_accuracy: 0.9771 - val_f1_m: 0.9299 - val_recall_m: 0.9279 - val_precision_m: 0.9324\n","Epoch 15/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9974 - f1_m: 0.9922 - recall_m: 0.9906 - precision_m: 0.9940\n","Epoch 15: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1035s 721ms/step - loss: 0.0077 - accuracy: 0.9974 - f1_m: 0.9922 - recall_m: 0.9906 - precision_m: 0.9940 - val_loss: 0.1219 - val_accuracy: 0.9733 - val_f1_m: 0.9188 - val_recall_m: 0.9173 - val_precision_m: 0.9204\n","Epoch 16/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9973 - f1_m: 0.9918 - recall_m: 0.9903 - precision_m: 0.9934\n","Epoch 16: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1035s 721ms/step - loss: 0.0084 - accuracy: 0.9973 - f1_m: 0.9918 - recall_m: 0.9903 - precision_m: 0.9934 - val_loss: 0.3090 - val_accuracy: 0.9548 - val_f1_m: 0.8644 - val_recall_m: 0.8642 - val_precision_m: 0.8646\n","Epoch 17/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9975 - f1_m: 0.9924 - recall_m: 0.9906 - precision_m: 0.9945\n","Epoch 17: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1032s 719ms/step - loss: 0.0073 - accuracy: 0.9975 - f1_m: 0.9924 - recall_m: 0.9906 - precision_m: 0.9945 - val_loss: 0.0503 - val_accuracy: 0.9857 - val_f1_m: 0.9565 - val_recall_m: 0.9557 - val_precision_m: 0.9575\n","Epoch 18/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976 - f1_m: 0.9927 - recall_m: 0.9913 - precision_m: 0.9943\n","Epoch 18: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1026s 714ms/step - loss: 0.0076 - accuracy: 0.9976 - f1_m: 0.9927 - recall_m: 0.9913 - precision_m: 0.9943 - val_loss: 0.0855 - val_accuracy: 0.9730 - val_f1_m: 0.9179 - val_recall_m: 0.9163 - val_precision_m: 0.9195\n","Epoch 19/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9974 - f1_m: 0.9922 - recall_m: 0.9903 - precision_m: 0.9942\n","Epoch 19: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1024s 713ms/step - loss: 0.0079 - accuracy: 0.9974 - f1_m: 0.9922 - recall_m: 0.9903 - precision_m: 0.9942 - val_loss: 0.0442 - val_accuracy: 0.9851 - val_f1_m: 0.9548 - val_recall_m: 0.9535 - val_precision_m: 0.9562\n","Epoch 20/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9979 - f1_m: 0.9938 - recall_m: 0.9925 - precision_m: 0.9952\n","Epoch 20: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1027s 715ms/step - loss: 0.0057 - accuracy: 0.9979 - f1_m: 0.9938 - recall_m: 0.9925 - precision_m: 0.9952 - val_loss: 0.0675 - val_accuracy: 0.9789 - val_f1_m: 0.9367 - val_recall_m: 0.9365 - val_precision_m: 0.9370\n","Epoch 21/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983 - f1_m: 0.9948 - recall_m: 0.9938 - precision_m: 0.9959\n","Epoch 21: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1019s 710ms/step - loss: 0.0053 - accuracy: 0.9983 - f1_m: 0.9948 - recall_m: 0.9938 - precision_m: 0.9959 - val_loss: 0.2012 - val_accuracy: 0.9597 - val_f1_m: 0.8783 - val_recall_m: 0.8772 - val_precision_m: 0.8794\n","Epoch 22/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9975 - f1_m: 0.9924 - recall_m: 0.9907 - precision_m: 0.9943\n","Epoch 22: val_f1_m did not improve from 0.96991\n","1436/1436 [==============================] - 1033s 719ms/step - loss: 0.0070 - accuracy: 0.9975 - f1_m: 0.9924 - recall_m: 0.9907 - precision_m: 0.9943 - val_loss: 0.0896 - val_accuracy: 0.9758 - val_f1_m: 0.9256 - val_recall_m: 0.9235 - val_precision_m: 0.9281\n","Epoch 23/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9978 - f1_m: 0.9932 - recall_m: 0.9921 - precision_m: 0.9946\n","Epoch 23: val_f1_m improved from 0.96991 to 0.97008, saving model to /content/drive/MyDrive/NIT Durgapur Internship 3/Output/model_best_ViT.h5\n","1436/1436 [==============================] - 1031s 718ms/step - loss: 0.0066 - accuracy: 0.9978 - f1_m: 0.9932 - recall_m: 0.9921 - precision_m: 0.9946 - val_loss: 0.0272 - val_accuracy: 0.9907 - val_f1_m: 0.9701 - val_recall_m: 0.9599 - val_precision_m: 0.9825\n","Epoch 24/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9980 - f1_m: 0.9940 - recall_m: 0.9925 - precision_m: 0.9956\n","Epoch 24: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1031s 718ms/step - loss: 0.0064 - accuracy: 0.9980 - f1_m: 0.9940 - recall_m: 0.9925 - precision_m: 0.9956 - val_loss: 0.1242 - val_accuracy: 0.9683 - val_f1_m: 0.9052 - val_recall_m: 0.9045 - val_precision_m: 0.9060\n","Epoch 25/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9984 - f1_m: 0.9953 - recall_m: 0.9942 - precision_m: 0.9964\n","Epoch 25: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1031s 718ms/step - loss: 0.0049 - accuracy: 0.9984 - f1_m: 0.9953 - recall_m: 0.9942 - precision_m: 0.9964 - val_loss: 0.2052 - val_accuracy: 0.9610 - val_f1_m: 0.8830 - val_recall_m: 0.8821 - val_precision_m: 0.8838\n","Epoch 26/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9979 - f1_m: 0.9936 - recall_m: 0.9921 - precision_m: 0.9951\n","Epoch 26: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1033s 719ms/step - loss: 0.0065 - accuracy: 0.9979 - f1_m: 0.9936 - recall_m: 0.9921 - precision_m: 0.9951 - val_loss: 0.1213 - val_accuracy: 0.9736 - val_f1_m: 0.9178 - val_recall_m: 0.9141 - val_precision_m: 0.9220\n","Epoch 27/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980 - f1_m: 0.9938 - recall_m: 0.9924 - precision_m: 0.9954\n","Epoch 27: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1031s 718ms/step - loss: 0.0062 - accuracy: 0.9980 - f1_m: 0.9938 - recall_m: 0.9924 - precision_m: 0.9954 - val_loss: 0.0931 - val_accuracy: 0.9741 - val_f1_m: 0.9221 - val_recall_m: 0.9218 - val_precision_m: 0.9225\n","Epoch 28/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9982 - f1_m: 0.9945 - recall_m: 0.9936 - precision_m: 0.9955\n","Epoch 28: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1030s 717ms/step - loss: 0.0060 - accuracy: 0.9982 - f1_m: 0.9945 - recall_m: 0.9936 - precision_m: 0.9955 - val_loss: 0.0646 - val_accuracy: 0.9780 - val_f1_m: 0.9311 - val_recall_m: 0.9277 - val_precision_m: 0.9353\n","Epoch 29/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9979 - f1_m: 0.9937 - recall_m: 0.9922 - precision_m: 0.9953\n","Epoch 29: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1027s 715ms/step - loss: 0.0062 - accuracy: 0.9979 - f1_m: 0.9937 - recall_m: 0.9922 - precision_m: 0.9953 - val_loss: 0.0448 - val_accuracy: 0.9857 - val_f1_m: 0.9564 - val_recall_m: 0.9555 - val_precision_m: 0.9575\n","Epoch 30/30\n","1436/1436 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9980 - f1_m: 0.9940 - recall_m: 0.9929 - precision_m: 0.9953\n","Epoch 30: val_f1_m did not improve from 0.97008\n","1436/1436 [==============================] - 1028s 716ms/step - loss: 0.0059 - accuracy: 0.9980 - f1_m: 0.9940 - recall_m: 0.9929 - precision_m: 0.9953 - val_loss: 0.1039 - val_accuracy: 0.9681 - val_f1_m: 0.8988 - val_recall_m: 0.8954 - val_precision_m: 0.9029\n"]}]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"1PyWLIFoaE8jfTsMYHcPtBfpQhg0Da42O","timestamp":1667932312620}],"machine_shape":"hm"},"environment":{"name":"tf2-gpu.2-4.m61","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}